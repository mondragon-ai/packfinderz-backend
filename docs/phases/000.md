## Title (number 0-N) => [PF-000]

Initialize Go monorepo with `cmd/api` and `cmd/worker` binaries

## Type

Infra

## Description

Create the PackFinderz Go monorepo foundation with two primary binaries: the HTTP API (`cmd/api`) and the async worker (`cmd/worker`). This establishes the build/deploy model (Heroku web + worker dynos) and the executable entrypoints for future modules (outbox publisher, schedulers, Pub/Sub consumers).

## Scope

* Create Go module and pin Go version
* Add `cmd/api/main.go` placeholder that boots and blocks
* Add `cmd/worker/main.go` placeholder that boots and blocks
* Ensure binaries compile independently (`go build ./cmd/api`, `go build ./cmd/worker`)

## Acceptance Criteria

* [ ] Repo builds with pinned Go version (e.g., `go.mod` + toolchain directive if used)
* [ ] `go build ./cmd/api` succeeds
* [ ] `go build ./cmd/worker` succeeds
* [ ] Both binaries run and exit cleanly on SIGINT/SIGTERM (placeholder shutdown hooks ok)

## Dependencies

* Blocked by: none
* Blocks: PF-001+

## Technical Notes

* **ASSUMPTION:** Go 1.22.x
* No business logic, routes, or workers implemented here.

## Out of Scope

* Any domain modules
* Any HTTP routing/middleware
* Any Pub/Sub integration

---

## Title (number 1-N) => [PF-001]

Define canonical project layout + module boundaries (`cmd/`, `api/`, `internal/`, `pkg/`)

## Type

Task

## Description

Establish the canonical PackFinderz repo structure and enforce ownership boundaries so shared code is discoverable (in `pkg/`) and domain code remains isolated (`internal/<domain>`). This prevents duplication of enums/types/helpers and avoids circular dependencies as the codebase grows.

## Scope

* Create directories:

  * `cmd/` (binaries)
  * `api/` (HTTP wiring: routes/controllers/middleware/validators/responses)
  * `internal/` (domain modules: services/repos/DTOs)
  * `pkg/` (global: enums/types/errors/infra helpers)
* Add documentation describing:

  * what goes in each folder
  * import rules (who can import whom)
  * where “global helpers” live (networking, geocoding, etc.)
* Add skeleton packages for:

  * `pkg/enums`, `pkg/types`, `pkg/errors`

## Acceptance Criteria

* [ ] Repo contains the canonical directory layout
* [ ] A README (or `docs/structure.md`) documents module boundaries and import rules
* [ ] `pkg/enums`, `pkg/types`, `pkg/errors` exist and are referenced as the canonical home for shared shapes

## Dependencies

* Blocked by: PF-000
* Blocks: PF-004, PF-005, PF-006, domain feature work

## Technical Notes

* Shared concerns SHOULD live in `pkg/*`; API-only concerns live in `api/*`.
* **ASSUMPTION:** tooling/docs live in `docs/` (or root README) as preferred.

## Out of Scope

* Implementing business logic or schemas
* Implementing any specific API endpoints beyond placeholders

---

## Title (number 2-N) => [PF-002]

Add env-only typed config loader used by API + worker

## Type

Task

## Description

Implement a typed configuration loader that reads from environment variables only, validates required values, and is shared by both `cmd/api` and `cmd/worker`. This is the single source of truth for runtime configuration.

## Scope

* Create `pkg/config` with:

  * typed `Config` struct (nested sections: DB, Redis, Auth/JWT, GCP, Logging, Runtime)
  * env parsing + validation
  * fail-fast behavior on missing/invalid required vars
* Provide a single constructor (e.g., `config.Load()`) used by both binaries
* Support “service account JSON as env text” pattern:

  * load JSON string from env
  * optionally write to temp file when a library requires a path (**ASSUMPTION:** required for Cloud SQL Proxy auth or GCP SDK in some cases)

## Acceptance Criteria

* [ ] `cmd/api` and `cmd/worker` both load config via `pkg/config`
* [ ] Process exits with clear error messages when required env vars are missing/invalid
* [ ] No config values are hard-coded in code

## Dependencies

* Blocked by: PF-001
* Blocks: PF-003, PF-004, PF-006, PF-009, PF-010, PF-016

## Technical Notes

* **ASSUMPTION:** configuration is not hot-reloaded in MVP.
* Avoid logging secrets; only log which keys are missing.

## Out of Scope

* Secrets manager integration (Heroku config vars / GCP Secret Manager handled later)
* `.env` file tooling (allowed locally but not committed)

---

## Define required env var manifest (dev vs prod)

## Type

Chore

## Description

Create a clear manifest of required environment variables for dev and prod that matches `pkg/config`. This is used for onboarding, CI, and deploy safety.

## Scope

* Add `ENV_VARS.md` (or `docs/env.md`) that:

  * lists all required env vars
  * indicates dev-only/prod-only
  * provides non-secret example formats
  * documents the “service account JSON as text” convention and temp-file behavior
* Ensure it stays synchronized with `pkg/config` fields

## Acceptance Criteria

* [ ] Env var manifest exists and matches config loader keys exactly
* [ ] Manifest clearly distinguishes dev vs prod variables
* [ ] No secrets are included in the manifest

## Dependencies

* Blocked by: PF-002
* Blocks: PF-015, PF-017

## Technical Notes

* Include sections for: DB, Redis, JWT/Auth, GCP, Logging, Runtime, Proxy settings.

## Out of Scope

* Automated env provisioning scripts

---

##  [PF-004] Implement structured JSON logger (global) with request/job correlation + optional stack traces

## Type

Task

## Description

Add a structured JSON logger usable across the entire codebase (API + workers), with request/job correlation and optional stack traces for warnings during debugging. This enables consistent observability and faster debugging without ad-hoc `fmt.Println`.

## Scope

* Create `pkg/observability/logger`:

  * initialization from config (level, dev/prod mode)
  * structured JSON output
  * `log.Error(ctx, msg, err)`-style API (ctx-aware)
  * helpers to attach fields: `request_id`, `user_id`, `store_id`, `actor_role`, etc.
* API integration:

  * middleware generates/propagates `request_id`
  * ensures all request logs include `request_id`
* Worker integration:

  * standard way to set “job context” fields (e.g., `request_id` from outbox payload when present)
* Stack traces:

  * included on `Error` by default
  * **optionally** attachable to `Warn` via config flag (debug mode)

## Acceptance Criteria

* [ ] Logs are JSON across API and workers (no unstructured default output)
* [ ] `request_id` is present on all HTTP request logs
* [ ] Worker logs can include `request_id` when available from upstream event payloads
* [ ] Stack traces appear on `Error`, and can be enabled for `Warn` via config toggle

## Dependencies

* Blocked by: PF-002, PF-001
* Blocks: PF-006, runtime debugging for all future work

## Technical Notes

* Location: `pkg/observability/logger` (per your preference).
* Recommend `zap` or `zerolog` (implementation choice).
* **ASSUMPTION:** `request_id` stored in `context.Context`.

## Out of Scope

* Log shipping/aggregation setup
* Distributed tracing (later; request_id propagation is the MVP baseline)

---

## [PF-005] Create global error code system + standard API response/error envelopes

## Type

Task

## Description

Implement a shared error model (global) with typed error codes + metadata (HTTP status, retryable, safe message), and provide API helpers that map domain errors into the canonical HTTP error envelope. This ensures consistent client behavior and avoids leaking internal errors.

## Scope

* Global error system in `pkg/errors`:

  * typed error code enum (or strongly-typed string type)
  * metadata: `http_status`, `retryable`, `public_message`, optional `details_allowed`
  * helper constructors: `errors.New(code, msg)`, `errors.Wrap(code, err, msg)`
* Global/shared response shapes in `pkg/types` as needed (only non-API-specific)
* API envelope helpers in `api/responses`:

  * success envelope helpers
  * error envelope writer that takes a `pkg/errors` error and produces:

    ```json
    { "error": { "code": "...", "message": "...", "details": {...} } }
    ```
* Define canonical mapping rules:

  * untrusted errors default to `500` with safe message
  * validation errors → `400`
  * auth errors → `401/403`
  * conflicts/state errors → `409/422`

## Acceptance Criteria

* [ ] `pkg/errors` defines typed error codes with metadata including HTTP status + retryable
* [ ] API can return canonical error envelope without leaking raw stack traces/messages
* [ ] At least one demo handler shows mapping from domain error → HTTP response using these helpers

## Dependencies

* Blocked by: PF-001
* Blocks: PF-006 and all endpoint implementations

## Technical Notes

* You indicated: “API should map domain errors → HTTP responses” — this ticket implements that pattern.
* **ASSUMPTION:** domain services return `pkg/errors`-typed errors (or wrap them) rather than ad-hoc errors.

## Out of Scope

* Localization/i18n
* Full error taxonomy for every endpoint (will expand organically as features land)

---

## [PF-006] Bootstrap Chi HTTP server + router groups with middleware, controllers, validators, responses

## Type

Task

## Description

Create the API HTTP server using Chi with a clear separation of concerns (`routes/`, `controllers/`, `middleware/`, `validators/`, `responses/`) and route groups for health/public/private/admin/agent endpoints. Middleware must attach per group (not via allowlists) and support the project’s auth + idempotency complexity.

## Scope

* Implement Chi server bootstrap in `cmd/api/main.go` (per your preference)
* Create folder structure:

  * `api/routes`
  * `api/controllers`
  * `api/middleware`
  * `api/validators`
  * `api/responses`
* Route groups (LOCKED):

  * `/health/*` (no auth)
  * `/api/public/*` (no auth)
  * `/api/private/*` (JWT + store context)
  * `/api/admin/*` (admin role)
  * `/api/v1/agent/*` (agent role)
* Middleware approach:

  * global: request_id, logging, panic recovery
  * per-group: JWT auth, store context enforcement, RBAC/role enforcement
  * placeholders for idempotency + rate limit middleware (wired but may be no-op until PF-010/PF-007)
* Controllers:

  * controllers register handlers but rely on validators + responses helpers
* Clear middleware ordering documented

## Acceptance Criteria

* [ ] Server starts and mounts all route groups
* [ ] `/health/live` and `/health/ready` routes are reachable without auth (handlers can be stubs until PF-008)
* [ ] `/api/private/*` rejects requests without JWT (401)
* [ ] `/api/admin/*` enforces admin role (403 if non-admin)
* [ ] `/api/v1/agent/*` enforces agent role (403 if non-agent)
* [ ] Middleware is attached per-router group (not centralized allowlist)

## Dependencies

* Blocked by: PF-004, PF-005
* Blocks: PF-007, PF-008, all endpoint implementations

## Technical Notes

* **ASSUMPTION:** JWT middleware initially validates signature/exp only; store membership checks can be added when auth module exists.
* Bootstrap stays localized in `cmd/api` to avoid premature abstraction.

## Out of Scope

* Implementing real auth flows/endpoints
* Implementing idempotency logic (wired later)

---

## [PF-007] Standard request validation layer for body + query (API)

## Type

Task

## Description

Implement a reusable request validation layer so all endpoints validate input early (before controller logic), reduce injection risk, and provide consistent `400` responses with details.

## Scope

* `api/validators`:

  * body parsing helpers (JSON)
  * query parsing helpers (typed parsing with defaults)
  * struct validation rules (e.g., required, min/max, email)
  * sanitization helpers where appropriate (e.g., trimming, normalizing)
* Integrate with `api/controllers` pattern:

  * validators called at start of each controller handler
  * controller receives validated typed input
* Standardized validation error mapping via `pkg/errors` + `api/responses`

## Acceptance Criteria

* [ ] Invalid JSON body returns `400` with canonical error envelope and field-level details
* [ ] Invalid query params return `400` with canonical error envelope
* [ ] Controllers follow a consistent pattern: validate → service → respond (no ad-hoc parsing scattered)

## Dependencies

* Blocked by: PF-006, PF-005
* Blocks: implementing API endpoints safely

## Technical Notes

* Recommend `go-playground/validator` (implementation choice).
* **ASSUMPTION:** sanitize “dangerous” strings minimally (trim, length caps) and rely on parameterized queries/ORM for SQL safety.

## Out of Scope

* Full anti-XSS/WAF solutions
* Rate limiting (separate ticket scope)

---

## [PF-008] Health endpoints for API (liveness + readiness with DB+Redis checks)

## Type

Infra

## Description

Implement health endpoints for the API binary. Liveness must only reflect process health, while readiness must check dependencies and report which dependency failed (Postgres, Redis, or both).

## Scope

* Add endpoints (no auth):

  * `GET /health/live`
  * `GET /health/ready`
* Liveness:

  * always `200` if process is running and router is serving
* Readiness:

  * checks Postgres connectivity
  * checks Redis connectivity
  * fails if either fails
  * response includes reason(s) and which dependency failed

## Acceptance Criteria

* [ ] `GET /health/live` returns 200 even if DB/Redis are down
* [ ] `GET /health/ready` returns non-200 if Postgres is down, Redis is down, or both
* [ ] `GET /health/ready` response includes explicit reasons (e.g., `postgres=false`, `redis=true`)
* [ ] Endpoints are mounted under `/health/*` and require no auth

## Dependencies

* Blocked by: PF-009, PF-010, PF-006
* Blocks: deployment readiness checks

## Technical Notes

* Per your guidance, this ticket is scheduled after DB/Redis integration.
* **ASSUMPTION:** readiness uses short timeouts (e.g., 250–500ms) to avoid hanging.

## Out of Scope

* Deep checks (Pub/Sub, GCS, BigQuery)
* Kubernetes-specific probe configs

---

## [PF-009]: Postgres bootstrap (Cloud SQL + Heroku-compatible) and dev proxy automation

## Type

Infra

## Description

Implement Postgres connectivity for API/worker with a dev experience that can automatically start Cloud SQL Proxy (ideally without manual `gcloud auth login`), while also supporting a straightforward TCP connection string for Heroku-style deployments.

## Scope

* `pkg/db` connection bootstrap that supports:

  * direct TCP DSN (e.g., `DATABASE_URL`) for Heroku/standard Postgres
  * Cloud SQL Proxy mode for dev (proxy runs locally and DB connects via localhost port)
* Dev proxy automation:

  * Make target starts Cloud SQL Proxy using service account JSON (provided via env; may be written to temp file)
  * No manual `gcloud auth login` required (service-account auth)
* Connection pooling, timeouts, and graceful shutdown hooks
* Expose a `Ping(ctx)`/health check function consumed by PF-008

## Acceptance Criteria

* [ ] API and worker can connect to Postgres via direct TCP DSN
* [ ] `make dev` (or a dedicated target) can start Cloud SQL Proxy using service-account auth without `gcloud auth login` (**best-effort; see assumptions**)
* [ ] DB bootstrap supports Cloud SQL via proxy for local dev
* [ ] DB health check returns explicit failure errors used by readiness endpoint

## Dependencies

* Blocked by: PF-002, PF-001
* Blocks: PF-012, PF-014, PF-008

## Technical Notes

* **ASSUMPTION:** Best practice for Heroku is direct TCP via `DATABASE_URL`. Cloud SQL connectivity in prod via proxy is optional; we will not force it if Heroku provides managed Postgres or external Postgres.
* **ASSUMPTION:** Cloud SQL Proxy can authenticate via service account JSON without interactive login.
* If Cloud SQL Proxy auth requires additional setup on your machine, document it in `ENV_VARS.md` and Make targets.

## Out of Scope

* Read replicas
* DB migrations execution policy (handled in PF-012)

---

## [PF-010] Redis bootstrap in `pkg/redis` for idempotency, rate limits, counters, and refresh-token sessions

## Type

Infra

## Description

Add a Redis client bootstrap in `pkg/redis` that supports the core infra use cases: idempotency, rate limiting, counters, and refresh-token session storage/rotation. This will power protected endpoints (idempotency) and auth session workflows.

## Scope

* `pkg/redis`:

  * client initialization (timeouts, pooling)
  * namespaced key builders (by concern: idempotency, rate_limit, counters, sessions)
  * helper functions for common patterns (GET/SET with TTL, SETNX, INCR)
* Refresh token session storage:

  * store refresh token string with TTL slightly beyond JWT expiry (per your model)
  * rotate refresh token when access token expires and refresh is used
  * ability to revoke refresh tokens (logout / forced rotation)
* Integrate Redis health checks used by PF-008 readiness
* Compatibility notes for Heroku Redis (URL-based auth)

## Acceptance Criteria

* [ ] API and worker can connect to Redis (Heroku Redis compatible)
* [ ] `pkg/redis` exposes helpers for:

  * idempotency storage (keyed by scope)
  * counters (INCR with TTL)
  * rate limiting primitives (token bucket or fixed window primitives)
  * refresh-token session set/get/revoke with TTL
* [ ] Redis health check function is usable by readiness endpoint

## Dependencies

* Blocked by: PF-002, PF-001
* Blocks: PF-008, idempotency middleware integration, auth session management

## Technical Notes

* **ASSUMPTION:** One Redis instance; logical separation via key prefixes.
* Refresh token details (exact key scheme) will align with the idempotency rules later (store scope includes userId/storeId where relevant).

## Out of Scope

* Redis cluster/sentinel
* Full auth implementation (endpoints/services)

---

## [PF-011]: Docker Compose for local Postgres + Redis

## Type

Infra

## Description

Provide a Docker Compose stack for local development dependencies (Postgres + Redis) with stable versions and persistent volumes, supporting a “local-only” mode without Cloud SQL.

## Scope

* `docker-compose.yml` (or `compose.yaml`) with:

  * Postgres service + volume
  * Redis service + volume (optional)
  * exposed ports for local dev
* Document how to run in local mode vs Cloud SQL Proxy mode

## Acceptance Criteria

* [ ] `docker compose up -d` starts Postgres and Redis
* [ ] Default local env values allow API/worker to connect
* [ ] Versions are pinned (no `latest`)

## Dependencies

* Blocked by: none
* Blocks: PF-015 (one-command dev)

## Technical Notes

* **ASSUMPTION:** Postgres image version aligns with Cloud SQL major version used.

## Out of Scope

* Running the API/worker as containers (optional later)

---

## [PF-012]: Migrations: `cmd/migrate` runner + hybrid policy (auto in dev, manual in prod)

## Type

Task

## Description

Implement migrations using Goose with a `cmd/migrate` runner and a hybrid execution policy: safe auto-migrate in dev (blocking startup on failure) and manual migrations in production. This addresses Heroku constraints and prevents accidental prod schema changes.

## Scope

* Create `cmd/migrate` binary that runs Goose migrations (`up`, optional `down`)
* Define migration execution policy:

  * Dev: API startup MAY auto-run migrations (blocking if failure)
  * Prod: migrations MUST be manual (via `cmd/migrate` invoked outside app startup)
* Add feature flag / env gate:

  * e.g., `DB_AUTO_MIGRATE=true|false` or `APP_ENV=dev|prod`
  * API uses this to decide whether to run migrations on startup
* Document Heroku strategy:

  * keep `cmd/migrate` in repo for local/CI/manual use
  * do not require Heroku to run it as a dyno type

## Acceptance Criteria

* [ ] `go build ./cmd/migrate` succeeds
* [ ] Local dev can run migrations via `make migrate-up` (calls `cmd/migrate`)
* [ ] In dev mode, API can be configured to auto-run migrations and fails startup if migrations fail
* [ ] In prod mode, API does NOT auto-run migrations by default
* [ ] Policy is documented in the deploy checklist/release notes ticket

## Dependencies

* Blocked by: PF-009, PF-002
* Blocks: PF-013, future schema work

## Technical Notes

* Addresses your prior Heroku issue by not requiring Heroku to “run migrate dyno”; keep it as a tool.
* **ASSUMPTION:** In prod, migrations are executed manually (local machine/CI job) against prod DB before deploying API changes.

## Out of Scope

* Full CD pipeline that runs migrations automatically on deploy

---

## [PF-013] Base migrations: enable `pgcrypto` + `postgis` extensions

## Type

Task

## Description

Add the initial Goose migrations to enable required Postgres extensions (`pgcrypto`, `postgis`) used by UUID generation and geo queries.

## Scope

* Create first/early migration(s) to:

  * `CREATE EXTENSION IF NOT EXISTS pgcrypto;`
  * `CREATE EXTENSION IF NOT EXISTS postgis;`
* Ensure migrations are safe to re-run

## Acceptance Criteria

* [ ] Running migrations enables both extensions successfully
* [ ] Migration is idempotent and re-runnable without error
* [ ] Verified via a simple SQL check or integration test step in dev docs

## Dependencies

* Blocked by: PF-012
* Blocks: schema/table migrations later

## Technical Notes

* Keep as early migration(s) to avoid later churn.

## Out of Scope

* Any table creation (will come in later phases)

---

## [PF-014]: GORM integration via `pkg/db` + base repo pattern (global DB instance, tx + raw SQL helpers)

## Type

Task

## Description

Integrate GORM (v2) using `pkg/db` as the initializer/exporter of the global `*gorm.DB`, with constructor injection into domain repositories and helpers for transactions and raw SQL where needed (e.g., atomic inventory updates).

## Scope

* In `pkg/db`:

  * initialize `*gorm.DB` using config/DSN (from PF-009/PF-002)
  * disable auto-migrate
  * configure connection pool + logger integration
  * expose:

    * `DB()` accessor (or return from bootstrap)
    * `WithTx(ctx, fn)` helper
    * raw SQL helpers (e.g., `Exec`, `Raw` wrappers with context)
* Define a base repository pattern used by future `internal/<domain>/repo` packages:

  * constructor takes `*gorm.DB`
  * consistent context passing

## Acceptance Criteria

* [ ] API and worker can initialize and share a single `*gorm.DB` instance
* [ ] Domain repos can receive `*gorm.DB` via constructor injection pattern (example skeleton)
* [ ] Transaction helper works and rolls back/commits correctly
* [ ] Auto-migrate is disabled (schema changes only through Goose)

## Dependencies

* Blocked by: PF-009, PF-002
* Blocks: domain repos + DB-backed services

## Technical Notes

* Yes: tx + raw SQL helpers are intended for use inside internal domain repos/services.
* **ASSUMPTION:** GORM v2; Postgres driver backed by `pgx` where possible.

## Out of Scope

* Defining full models/tables (comes later)
* Implementing any specific repositories beyond pattern skeleton

---

## Title (number 15-N) => [PF-015]

Makefile + dev tooling: one-command startup (background), proxy, migrate, api, worker

## Type

Chore

## Description

Create a Makefile that provides a consistent developer workflow, including a single target to start dependencies and app components in the background (preferably via docker compose + optional Cloud SQL Proxy), plus standard targets for build/test/lint/migrate.

## Scope

* Add Make targets:

  * `make dev` (starts deps + api + worker; background where possible)
  * `make down` (stops dev stack)
  * `make logs` (tail relevant logs)
  * `make build`, `make test`, `make lint`
  * `make migrate-up`, `make migrate-down` (if supported)
* Refactor Cloud SQL Proxy start/stop into reusable targets
* Support local mode:

  * docker compose Postgres + Redis
* Support Cloud SQL mode:

  * start proxy if configured
* Document expected workflow in README

## Acceptance Criteria

* [ ] `make dev` starts docker compose dependencies and launches API + worker without blocking the terminal (best-effort)
* [ ] `make down` stops all services started by `make dev`
* [ ] `make migrate-up` runs Goose migrations via `cmd/migrate`
* [ ] Dev workflow is documented and reproducible

## Dependencies

* Blocked by: PF-011, PF-009, PF-010, PF-012
* Blocks: PF-016, contributor onboarding velocity

## Technical Notes

* “Background + logs via docker” is preferred; if API/worker run outside docker, provide `make logs-api`/`make logs-worker` best-effort.
* **ASSUMPTION:** macOS/Linux primary; Windows not required.

## Out of Scope

* Full dockerized app runtime (optional later)

---

## [PF-016]: Single GitHub Actions CI pipeline: lint, test, build, secret scanning (DB tests skipped for now)

## Type

Infra

## Description

Add a single GitHub Actions workflow that enforces code quality and blocks merges on failures. Include secret scanning gates. Skip DB-dependent tests for now to avoid early pipeline friction.

## Scope

* Create a single workflow (e.g., `.github/workflows/ci.yml`) that runs on:

  * pull requests
  * pushes to main
* Steps:

  * checkout
  * setup Go (pinned)
  * restore cache
  * `go fmt` / formatting check
  * `golangci-lint` (or equivalent)
  * `go test ./...` (DB-dependent tests excluded/skipped)
  * `go build ./...` (or at least `cmd/api`, `cmd/worker`, `cmd/migrate`)
  * secret scanning (e.g., gitleaks) with fail gate
* Enforce required checks (document in repo settings notes)

## Acceptance Criteria

* [ ] One workflow exists and runs on PRs
* [ ] CI fails on lint/test/build failure
* [ ] CI fails if secrets are detected
* [ ] DB-dependent tests are skipped or isolated (documented)

## Dependencies

* Blocked by: PF-015
* Blocks: safe team scaling / PR gating

## Technical Notes

* **ASSUMPTION:** DB tests are tagged (e.g., build tags) or structured so they can be excluded until infra is ready.
* Keep workflow single as requested.

## Out of Scope

* CD to production
* Automated migrations against prod DB

---

## [PF-017]: Heroku Procfile (web + worker) + release notes + deploy checklist (hybrid migrations)

## Type

Infra

## Description

Add Heroku deployment wiring for API and worker dynos and document an operational release/deploy checklist including the hybrid migration policy (manual in prod, optional auto in dev).

## Scope

* Add `heroku.yml`:

  * `web: ./bin/api` (or equivalent buildpack output)
  * `worker: ./bin/worker`
* Ensure checklist mentions:

  * readiness endpoints
  * what to verify post-deploy

## Acceptance Criteria

* [ ] Procfile exists and runs API + worker on Heroku
* [ ] Deploy checklist explicitly documents prod migration approach and sequence
* [ ] Release notes template exists and is usable for each deploy

## Dependencies

* Blocked by: PF-000, PF-003, PF-015
* Blocks: first deploy readiness

## Technical Notes

* **ASSUMPTION:** buildpack produces binaries into a known path; Procfile commands match that.
* Keep `cmd/migrate` as a tool; do not require Heroku dyno type for it.

## Out of Scope

* Staging environment
* Automated CD pipeline
