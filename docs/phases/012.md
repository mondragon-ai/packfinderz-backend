## **Title [PF-1XX]: Provision BigQuery datasets and tables for marketplace analytics**

## **Type**

Infra

## **Description**

Create and provision the BigQuery infrastructure required for marketplace analytics. This includes datasets and tables for event ingestion, with correct partitioning and clustering to support time-series queries and aggregation at scale.

## **Scope**

* Create BigQuery dataset (e.g. `packfinderz_analytics`)
* Create tables:

  * `marketplace_events`
  * `ad_events`
* Apply partitioning and clustering rules aligned with query patterns

## **Acceptance Criteria**

* [ ] Dataset exists in the configured GCP project
* [ ] `marketplace_events` table exists and is time-partitioned
* [ ] `ad_events` table exists and is time-partitioned
* [ ] Tables are clustered on relevant dimensions (e.g. store_id, vendor_id, event_type)
* [ ] Schema is versioned or documented in code

## **Dependencies**

* Blocked by: GCP project + BigQuery API enabled
* Blocks: PF-1XX, PF-1XX

## **Technical Notes**

* Execution MAY be split between:

  * `gcloud` CLI / Terraform-style scripts
  * Go-based bootstrap code
* Recommended fields (marketplace_events):

  * `event_id (string)`
  * `event_type (string)`
  * `order_id (string, nullable)`
  * `store_id (string, nullable)`
  * `vendor_store_id (string, nullable)`
  * `amount_cents (int, nullable)`
  * `occurred_at (timestamp)`
  * `ingested_at (timestamp)`
* **ASSUMPTION:** schema evolution handled manually in MVP

## **Out of Scope**

* Backfilling historical data
* BI dashboards or Looker setup

---

## **Title [PF-110]: Implement BigQuery client bootstrap and readiness checks**

## **Type**

Task

## **Description**

Add a reusable BigQuery client under `pkg/bigquery` to support ingestion and querying. The client must perform startup/readiness checks to ensure the BigQuery API and datasets are accessible before processing events. integrate intpo th eping check as well. 

## **Scope**

* Implement BigQuery client wrapper
* Add dataset/table existence checks 
* Integrate with API and/or worker(s) startup

## **Acceptance Criteria**

* [ ] BigQuery client initializes with service account credentials -> follow GCS & pubsub clients
* [ ] Readiness check fails fast if dataset or tables are missing
* [ ] Client is reusable by workers and API services

## **Dependencies**

* Blocked by: PF-1XX
* Blocks: PF-1XX, PF-1XX, PF-1XX

## **Technical Notes**

* Location: `pkg/bigquery`
* Should expose:

  * `InsertRows(ctx, table, rows)`
  * `Query(ctx, sql, params)`
* **ASSUMPTION:** credentials are provided via standard GCP env vars
* Table Schemas Documented here in JSON: `infra/bigquery/schemas/v1/ad_events.schema.json` & `infra/bigquery/schemas/v1/marketplace_events.schema.json`

## **Out of Scope**

* Retry/backoff policies beyond default SDK behavior
* Multi-project or multi-dataset support

---

## **Title [PF-1XX]: Analytics ingestion consumers (outbox → BigQuery)**

## **Type**

Feature

## **Description**

Implement analytics consumers that process outbox events and insert corresponding rows into BigQuery. These consumers translate domain events into analytics records and ensure exactly-once semantics via idempotency keys.

## **Scope**

* Add consumers under `internal/consumers/analytics`
* Handle events:

  * `order_created`
  * `cash_collected`
  * `order_paid`
* Insert rows into `marketplace_events`
* Enforce per-consumer idempotency

## **Acceptance Criteria**

* [ ] Each supported event type inserts exactly one BigQuery row
* [ ] Duplicate outbox deliveries do not create duplicate rows
* [ ] Failures are logged and retry-safe

## **Dependencies**

* Blocked by: PF-1XX, PF-1XX
* Blocks: PF-1XX, PF-1XX

## **Technical Notes**

* Integrated into existing outbox worker scheduler
* Pattern:

  * Switch on `event.type`
  * Dispatch to analytics consumer
* Idempotency key format:

  * `pf:evt:processed:analytics:<event_id>`
* **ASSUMPTION:** outbox events include sufficient metadata for analytics rows

## **Out of Scope**

* Ad analytics ingestion (handled separately)
* Reprocessing or replay tooling

---

## **Title [PF-1XX]: Vendor analytics API endpoint**

## **Type**

Feature

## **Description**

Expose a vendor-facing analytics endpoint that queries BigQuery to return KPIs and time-series data for the active vendor store.

## **Scope**

* Implement `GET /api/v1/vendor/analytics`
* Support time presets (e.g. 7d, 30d, 90d)
* Return:

  * KPIs (orders, revenue, AOV, cash collected)
  * Time-series aggregates

## **Acceptance Criteria**

* [ ] Endpoint returns data scoped to vendor’s store only
* [ ] Time preset parameter affects query range
* [ ] Results are sourced from BigQuery, not Postgres
* [ ] Non-vendor access is rejected

## **Dependencies**

* Blocked by: PF-1XX, PF-1XX
* Blocks: None

## **Technical Notes**

* Controller: `api/controllers/analytics/vendor`
* Queries SHOULD use parameterized SQL
* **ASSUMPTION:** predefined presets only (no arbitrary date ranges in MVP)

## **Out of Scope**

* Custom dashboards or filters
* Real-time analytics guarantees

---

## **Title [PF-1XX]: Admin analytics API endpoint**

## **Type**

Feature

## **Description**

Expose an admin-only analytics endpoint providing global marketplace KPIs derived from BigQuery. This endpoint supports operational and financial oversight.

## **Scope**

* Implement `GET /api/v1/admin/analytics`
* Return global KPIs across all stores
* Enforce admin-only access

## **Acceptance Criteria**

* [ ] Endpoint returns aggregated marketplace metrics
* [ ] Queries scan BigQuery tables only
* [ ] Non-admin access returns 403

## **Dependencies**

* Blocked by: PF-1XX, PF-1XX
* Blocks: None

## **Technical Notes**

* Controller: `api/controllers/analytics/admin`
* Metrics MAY include:

  * total GMV
  * total orders
  * cash collected
  * payouts completed
* **ASSUMPTION:** no per-store breakdown required in MVP

## **Out of Scope**

* Exporting analytics
* Scheduled reports or alerts
