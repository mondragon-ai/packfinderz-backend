## [PF-033] Implement store profile read endpoint (active store)

## Type

Feature

## Description

Implement an authenticated endpoint to fetch the **active store’s full profile** based on `activeStoreId` from JWT. This endpoint returns **non-public internal fields** and is used for store settings.

## Scope

* Fetch store by `activeStoreId`
* Return full store profile.
* Enforce store-scoped authorization

## Acceptance Criteria

* [ ] `GET /api/v1/stores/me` returns store data for `activeStoreId`
* [ ] Request without `activeStoreId` && valid JWT is rejected
* [ ] Buyer and vendor stores are both supported
* [ ] Response omits any fields not allowed to be read (e.g., internal-only flags if any)

## Dependencies

* Blocked by: Auth + JWT with `activeStoreId`
* Blocks: Store settings UI, store update endpoint

## Technical Notes

* Table: `stores`
* Authorization: store-scoped
* **ASSUMPTION:** Address is returned but not editable in MVP

## Out of Scope

* Public vendor directory fields
* Store analytics data

---

## [PF-034] Implement store profile update endpoint (non-address fields)

## Type

Feature

## Description

Implement an endpoint to update **mutable store fields only** for the active store. Address fields are immutable in MVP and must not be modifiable via this endpoint.

## Scope

* Update allowed store fields (e.g., `description`, `phone`, `email`, `social`, `company_name`, `banner_url`, `logo_url` etc)
* Enforce role-based access (owner/manager only)
* Prevent address or geo changes

## Acceptance Criteria

* [ ] `PUT /api/v1/stores/me` updates allowed fields only
* [ ] Attempt to modify address or geo fields is rejected
* [ ] Only authorized roles may update store
* [ ] Updated timestamps reflect changes

## Dependencies

* Blocked by: [PF-033]
* Blocks: Store settings management

## Technical Notes

* Table: `stores`
* Validation required at API layer
* **ASSUMPTION:** Address mutability handled only via admin override later
* We need to create a new goose migrations to add these fields to a store:  `banner_url` (string), `logo_url` (string), `ratings` (map[string]int), `Categories` (string[])

## Out of Scope

* Admin override of store address
* Subscription or KYC status updates

---

## [PF-035] Implement store user list endpoint

## Type

Feature

## Description

Implement an endpoint to list **all users (memberships)** associated with the active store. Used for store user management.

## Scope

* Fetch memberships for `activeStoreId`
* Include user basic profile + role
* Enforce store-scoped access  (`managers` & `owner` roles can access)

## Acceptance Criteria

* [ ] `GET /api/v1/stores/me/users` returns memberships for active store
* [ ] Only authorized roles can access this endpoint (`managers` & `owner`)
* [ ] Each entry includes user id, email, name (first & last name), role, created_at, last_login_at, member_role, membership_status (pulled from store_member row and may join from User table)

## Dependencies

* Blocked by: Store membership model
* Blocks: Invite/remove user flows

## Technical Notes

* Tables: `store_memberships`, `users`
* **ASSUMPTION:** Pagination optional for MVP due to low cardinality

## Out of Scope

* Cross-store user search
* Role modification (future)

---

## [PF-036]: Implement invite user endpoint (temporary password)

## Type

Feature

## Description

Implement an endpoint to invite a user to the active store by email and role. If the user does not exist, create them with a **temporary password** and add store membership.

## Scope

* Accept email + role
* Create user if not exists
* Create store membership
* Idempotent behavior

## Acceptance Criteria

* [ ] `POST /api/v1/stores/me/users/invite` creates membership
* [ ] Existing users are reused (no duplicate users)
* [ ] Temporary password is generated (not logged) hashed and stored like a normal pass. The user will log in with the pass given and change it in their settings. 
* [ ] Duplicate invite returns idempotent success

## Dependencies

* Blocked by: Users + memberships
* Blocks: Team management

## Technical Notes

* Tables: `users`, `store_memberships`
* **ASSUMPTION:** Temp password is communicated out-of-band
* **ASSUMPTION:** Email sending not implemented yet
* **ASSUMPTION:** Outbox event and notifcation subscriber worker not implemented yet

## Out of Scope

* Email delivery
* Invite acceptance flow

---

## [PF-037]: Implement remove store user endpoint

## Type

Feature

## Description

Implement endpoint to remove a user from the active store by deleting the store membership only.

## Scope

* Delete `store_memberships` row
* Prevent removing last owner
* Enforce authorization

## Acceptance Criteria

* [ ] `DELETE /api/v1/stores/me/users/{userId}` removes membership
* [ ] Removing last owner is rejected
* [ ] User account itself is not deleted

## Dependencies

* Blocked by: [PF-035]
* Blocks: Store user management completeness

## Technical Notes

* Table: `store_memberships`
* **ASSUMPTION:** Last-owner constraint enforced at API layer

## Out of Scope

* Deleting users globally
* Role reassignment

---

Here is the **single consolidated ticket** for **Phase 2 — Media Metadata Persistence (Postgres)**, written **exactly in the style and rigor of your example**.

---

## [PF-038]: Implement canonical `Media` metadata persistence (Postgres + GORM)

## Type

Task

## Description

Implement the canonical **Media metadata persistence layer** for PackFinderz. This includes authoritative Postgres tables for uploaded media objects and their attachments to domain entities. All media metadata MUST be stored in Postgres via Goose SQL migrations and mapped via GORM models. This phase establishes the database and repository foundation for all GCS-backed uploads (licenses, COAs, manifests, product media, ads, profile pictures, etc).

This ticket does **not** implement upload or presigned URL flows; it strictly defines the metadata layer used by those flows.

## Scope

* Implement Goose SQL migration to create `media` table (per Data Design)
* Define canonical `media_kind` enum values
* Implement GORM models for `Media`
* Implement base media repository with create, fetch, delete, and attach semantics

## Acceptance Criteria

* [ ] Goose migration creates `media` table with:

  * `id` (uuid, pk)
  * `store_id` (uuid, not null)
  * `user_id` (uuid, not null)
  * `kind` (`media_kind` enum)
  * `gcs_key` (text unique) // This is the key to reference the GCS object
  * `file_name` (text)
  * `mime_type` (text)
  * `ocr` (text nullable) // the OCR text file reference if avaialble
  * `size_bytes` (bigint)
  * `is_compressed` (boolean, default false)
  * `created_at` (timestamptz)

* [ ] Canonical `media_kind` enum includes:

  * `product` (product images, profile/store images, ads)
  * `ads` (product videos, ad creatives)
  * `pdf`
  * `license_doc`
  * `coa`
  * `manifest`
  * `user`
  * `other`
* [ ] GORM models exist for `Media` in the DB models folder
* [ ] Base media repository supports:

  * create media record
  * fetch media by ID
  * delete media (later ticket will also delete the GSC instance)
* [ ] No storage URLs or secrets are derived or mutated in this layer

## Dependencies

* Blocked by: Phase 0 infra (Postgres + Goose runner)
* Blocks:

  * Presigned upload flow
  * Media finalize endpoint
  * License upload
  * Product / ad media attachments
  * Manifest storage

## Technical Notes

* Tables: `media`
* Enums: `media_kind` (Postgres enum)
* Ownership:

  * `media.store_id` is optional but preferred for store-scoped uploads
  * `media.user_id` tracks uploader when available
* Repository MUST accept injected `*gorm.DB` and be context-aware
* **ASSUMPTION:** Hard deletes are allowed only when no attachments exist

## Out of Scope

* GCS client setup
* Presigned upload or download URLs
* Media compression or OCR
* Background processing workers
* Public vs private media access rules

---

## [PF-039]: Bootstrap Google Cloud Storage (GCS) client for API and worker

## Type

Task

## Description

Implement a **canonical, reusable Google Cloud Storage (GCS) client bootstrap** for PackFinderz and wire it into both the API and worker binaries. GCS must be treated as a first-class infrastructure dependency, similar to `pkg/db` and `pkg/redis`.

The GCS client MUST live under `pkg/storage` (with sub-packages as needed) and support **context-aware injection**, **multiple authentication strategies**, and **multi-bucket usage**. Both `cmd/api` and `cmd/worker` MUST initialize and reuse the same client abstraction. Readiness probes MUST fail if GCS is unavailable or misconfigured.

## Scope

* Implement canonical GCS client bootstrap under `pkg/storage`
* Support authentication via:

  * Service account JSON (from env / file)
  * Default credentials / IAM permissions (gcloud / workload identity)
* Read bucket configuration from env (per existing ENV file)
  ```env
    PACKFINDERZ_GCS_BUCKET_NAME=packfinderz-media
    PACKFINDERZ_GCS_UPLOAD_URL_EXPIRY=15m
    PACKFINDERZ_GCS_DOWNLOAD_URL_EXPIRY=30m
  ```
* Wire GCS client initialization into:

  * `cmd/api` startup
  * `cmd/worker` startup
* Register GCS as a required dependency in health/readiness checks

## Acceptance Criteria

* [x] A `pkg/storage/gcs` package exists with:

  * Context-aware client creation
  * Clean public interface (no direct usage of `storage.NewClient` outside this package)
* [x] GCS client supports **both** auth modes:

  * Service account JSON credentials (explicit)
  * Implicit credentials via gcloud / IAM
* [x] Bucket names are injected via env vars from the config files in pkg (no hardcoded bucket names)
* [x] API binary initializes GCS client during startup and fails fast on error
* [x] Worker binary initializes GCS client during startup and fails fast on error
* [x] GCS dependency is included in `/health/ready` checks
* [x] No credentials, bucket names, or object paths are logged
* [x] GCS client is shared via dependency injection (not recreated per request)

* **Implementation:** new `pkg/storage/gcs` bootstrap + `NewClient`, API/worker initialization, `/health/ready` dependency check, and shared routing.

## Dependencies

* Blocked by: Phase 0 infra (GCP project, buckets, IAM permissions)
* Blocks:

  * Media presigned upload
  * Media finalize flow
  * License / COA / manifest / Media (pic/video) storage
  * Media processing worker

## Technical Notes

* Package structure (suggested):

  ```
  pkg/
    storage/
      client.go        // client bootstrap + options
      buckets.go       // typed bucket handles (from config + constants file from pkg)
      health.go        // readiness checks (used in the health api)
      errors.go        // wrapped, typed errors (use the pkg/errors though)
  ```

* Auth strategy:

  * If explicit service account JSON env var is present → use it
  * Else fall back to default credentials (gcloud / workload identity)

* Client MUST accept `context.Context`
* GCS client MUST be created once at startup and reused
* Health check MAY perform a lightweight operation (e.g. bucket attrs)
* **ASSUMPTION:** Bucket names are already defined in env
* Probably need to use `cloud.google.com/go/storage`


## Out of Scope

* Presigned upload/download URLs
* Media metadata persistence
* Object lifecycle rules
* Public vs private bucket policy
* Media compression / OCR

---

## [PF-040] Create `media` table + enum + GORM model (upload lifecycle)

## Type

Feature

## Description

Modify the canonical `media` persistence model that tracks uploaded objects across the platform, including lifecycle state, ownership, and metadata. This table is the **source of truth** for all uploaded files and their processing state.

## Scope

* Create `media_status` enum
* Create `media` table with lifecycle fields + timestamps
* Modify GORM `Media` model aligned with schema
* Add if not present indexes and constraints for idempotency and lookup

## Acceptance Criteria

* [ ] Postgres enum `media_status` exists with values:
  `pending | uploaded | processing | ready | failed | delete_requested | deleted | delete_failed`
* [ ] `media` table exists with:

  * `id uuid pk`
  * `store_id uuid not null`
  * `user_id uuid not null`
  * `kind media_kind not null`
  * `status media_status not null default 'pending'`
  * `gcs_key text not null unique`
  * `file_name text not null`
  * `mime_type text not null`
  * `size_bytes bigint not null`
  * `ocr text null`
  * `is_compressed boolean not null default false`
  * timestamps:
    `created_at`, `updated_at`,
    `uploaded_at`, `verified_at`,
    `processing_started_at`, `ready_at`,
    `failed_at`, `deleted_at`
* [ ] Indexes:

  * `unique(gcs_key)`
  * `(store_id, created_at desc)`
* [ ] GORM model matches schema exactly
* [ ] Goose migration is reversible

## Dependencies

* Blocked by: DB bootstrap
* Blocks: presign endpoint, workers, media reads

## Technical Notes

* `gcs_key` format:
  `<STORE_ID>/<MEDIA_KIND>/<MEDIA_ID>.<ext>`
* Bucket name is **not stored** (env-level concern)

## Out of Scope

* Upload endpoints
* Signed URLs
* Workers

---

## [PF-041] Implement `POST /api/v1/media/presign` (create media row + signed PUT)

## Type

Feature

## Description

Allow authenticated users to initiate uploads by generating a signed GCS PUT URL. This endpoint **creates the media row first**, assigns a deterministic GCS object key, and returns a signed URL for direct client upload.

This is the **only API call required before upload**.

## Scope

* Endpoint: `POST /api/v1/media/presign`
* Validate:

  * auth + activeStoreId
  * role permissions
  * media_kind
  * mime_type
  * size_bytes ≤ 20MB
* Generate:

  * `media_id`
  * `gcs_key`
* Insert `media` row with `status = pending`
* Generate signed PUT URL with:

  * exact `Content-Type` enforcement
  * short TTL
* Enforce idempotency via `Idempotency-Key`

## Acceptance Criteria

* [ ] Media row is created **before** upload begins
* [ ] Same idempotency key returns same `{media_id, gcs_key, signed_put_url}`
* [ ] `gcs_key` always includes `media_id`
* [ ] Signed PUT requires matching `Content-Type`
* [ ] No upload proxying through API
* [ ] Logs never include signed URLs or raw GCS keys

## Dependencies

* Blocked by: PF-040, PF-039
* Blocks: upload completion via Pub/Sub

## Technical Notes

* Allowed mime mapping derived from `media_kind` enum
* Max size: **20MB global**
* No checksum support (explicit)

## Out of Scope

* Multipart uploads
* Resumable uploads
* Media processing

---


## [PF-042]: Convert cmd/worker startup to bootstrap full dependencies (config, logger, DB, Redis, Pub/Sub, GCS)

## Type

Task

## Description

Update `cmd/worker` to initialize the same first-class dependencies as the API (per MASTER CONTEXT): config, structured logger, GORM DB, Redis, Pub/Sub client/subscriber, and GCS client. This makes the worker capable of consuming Pub/Sub events and performing DB updates reliably in prod (Heroku worker dyno).

## Scope

* Load env config for worker (including Pub/Sub + GCS + DB + Redis)
* Create/initialize:

  * `pkg/logger` logger
  * `pkg/db` GORM bootstrap + readiness ping
  * `pkg/redis` client + readiness ping
  * Pub/Sub client (project-scoped)
  * GCS client (`pkg/storage/gcs`) (already exists)
* Define worker “service kind” behavior (so worker runs consumers / loops)
* Add `health/ready`-style checks for dependencies the worker requires (at minimum DB + Pub/Sub)

## Acceptance Criteria

* [ ] `cmd/worker` boots successfully with all required env vars present and logs dependency init success
* [ ] Worker exits with a clear error if any required dependency cannot initialize (DB, Pub/Sub, Redis, GCS)
* [ ] Worker exposes (or logs) readiness checks consistent with existing patterns (Ping helpers)
* [ ] No API-only middleware/router is pulled into worker (clean separation)

## Dependencies

* Blocked by: none
* Blocks: Pub/Sub consumer + media status updater (Ticket 3)

## Technical Notes

* Reuse existing patterns described in MASTER CONTEXT:

  * `pkg/db` bootstrap + `Ping`
  * `pkg/redis` bootstrap
  * `pkg/storage/gcs` client bootstrap (already pinged successfully)
* Config additions likely needed (names illustrative; align to your config package conventions):

  * `PACKFINDERZ_GCP_PROJECT_ID`
  * `PACKFINDERZ_PUBSUB_MEDIA_SUBSCRIPTION` (e.g. `gcp-meda-sub`)
  * `PACKFINDERZ_PUBSUB_MEDIA_TOPIC` (optional if only subscribing)
* ASSUMPTION: Worker uses a long-running process model (not cron) for Pub/Sub consumption.

## Out of Scope

* Media post-processing pipelines (compression/OCR) beyond status updates
* Outbox publisher loop changes (separate concern)

---

## [PF-043]: Implement Pub/Sub consumer for GCS OBJECT_FINALIZE to mark Media rows as uploaded

## Type

Feature

## Description

Consume `OBJECT_FINALIZE` notifications from Pub/Sub (`gcp-meda-sub`), extract the object key (`name`), map it to `media.gcs_key`, load the Media row, and update lifecycle fields to reflect a successful upload (status + timestamps). This is the first worker-side step in the media pipeline.

## Scope

* Create a worker consumer module (e.g. `internal/media/consumer` or similar)
* Subscribe to `gcp-meda-sub` and handle messages:

  * Parse JSON payload (GCS notification)
  * Extract `bucket` + `name` (objectId/name)
  * Lookup `Media` by `GCSKey == name`
  * Update:

    * `Status = uploaded`
    * `UploadedAt = now()`
    * `UpdatedAt` auto
* Idempotency:

  * If media already `uploaded|processing|ready`, treat as success (no-op)
* Error handling:

  * If Media row not found: log warn + ACK (do not poison queue)
  * If DB transient error: NACK / retry
* Logging:

  * Include `request_id`-like correlation (message_id) + media_id + gcs_key
* Add minimal tests for payload parsing + state transition logic (unit tests)

## Acceptance Criteria

* [ ] Uploading an object whose key equals an existing `media.gcs_key` results in that row being updated to `status=uploaded` and `uploaded_at` set
* [ ] Re-delivered duplicate Pub/Sub messages do not break correctness (idempotent updates)
* [ ] If no media row exists for the object key, the worker logs a warning and ACKs the message
* [ ] If DB update fails transiently, the message is retried (NACK) up to the configured policy

## Dependencies

* Blocked by: worker dependency bootstrap (Ticket 2)
* Blocks: later media processing pipeline (compression/OCR) (future tickets)

## Technical Notes

* Your message payload from logs includes:

  * `bucket: "packfinderz-media"`
  * `name: "test/gcs-test.txt"` (this is the object key; should match `Media.GCSKey`)
  * `eventType: "OBJECT_FINALIZE"`
* ASSUMPTION: `MediaStatus` enum includes `pending` and `uploaded` (or equivalent). If not, add migration/enum value separately.
* ASSUMPTION: We will not require `/media/finalize` to mark uploaded; Pub/Sub is authoritative for “object exists”.

## Out of Scope

* Compression / OCR / thumbnailing
* Verifying object integrity beyond existence (e.g., size/hash verification)
* Any new API endpoints

---


## [PF-045]: Add DLQ + retry policy for media Pub/Sub consumer and structured failure logging

## Type

Infra

## Description

Harden the media consumer so poison messages don’t cause infinite retries and so operational issues are visible. Add explicit retry bounds, dead-letter routing, and consistent structured logs for failures.

## Scope

* Define retry behavior (max attempts) and DLQ subscription/topic for media events
* Configure subscription policy:

  * DLQ topic
  * max delivery attempts
* Update worker consumer to:

  * Log attempt count / delivery attempt
  * Emit error logs with safe fields (no PII, no doc contents)
* Add a short runbook for replaying DLQ messages (manual MVP)

## Acceptance Criteria

* [ ] Poison messages exceed max attempts and land in DLQ (not infinite retry)
* [ ] Worker logs include message_id, deliveryAttempt (if available), gcs_key, media_id (if found), error_code
* [ ] Runbook exists to inspect DLQ and replay (manual)

## Dependencies

* Blocked by: Ticket 3 consumer baseline
* Blocks: production readiness for media pipeline

## Technical Notes

* Pub/Sub delivery attempt is available when using exactly-once / dead-letter features; if not available in handler context, log what you can and still DLQ via subscription config.
* ASSUMPTION: We will keep MVP replay manual (no automated replay tool yet).

## Out of Scope

* Full observability dashboards (metrics/tracing) beyond logs
* Terraform management for DLQ resources

---

## [PF-046]: Add local/dev “media upload event” verification harness (CLI + optional Go script)

## Type

Chore

## Description

Create a lightweight harness to validate the end-to-end flow locally or in dev: presign → upload to GCS → Pub/Sub event → worker updates DB row. This prevents regressions and makes onboarding faster.

## Scope

* Document the exact steps:

  * Create Media row (via presign endpoint already built)
  * Upload file to signed URL
  * Confirm Pub/Sub message received
  * Confirm DB row transitions to uploaded
* Optional: add a Go script under `cmd/devtools/media_e2e` that:

  * Uploads a sample file to a provided signed URL
  * Pulls one Pub/Sub message (or relies on running worker)
  * Verifies DB status (requires DSN)
* Ensure docs do not leak secrets

## Acceptance Criteria

* [ ] A developer can reproduce “upload → worker updates Media” in <10 minutes following the doc
* [ ] The harness clearly states required env vars and expected outputs/log lines
* [ ] No sensitive values are printed (signed URLs OK; no tokens/credentials in logs)

## Dependencies

* Blocked by: Ticket 3
* Blocks: none

## Technical Notes

* Use your existing verified CLI flow as the baseline (the `gsutil cp` + `gcloud pubsub subscriptions pull` pattern).
* ASSUMPTION: DB is reachable from dev environment (Heroku PG or local compose).

## Out of Scope

* Automated CI integration for this e2e (future)
* Compression/OCR validation (future)


---
