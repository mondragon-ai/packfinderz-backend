# Title [PF-182]: Refactor BigQuery schemas for marketplace_events and ad_event_facts

## Type

Infra

## Description

Update the BigQuery table schemas to match the canonical Analytics Engine spec so the analytics-worker can write query-friendly, flattened rows for marketplace KPIs and ad KPIs. This replaces the older “payload-only” style for marketplace events with required flattened columns plus embedded `items` JSON for Top Products/Categories queries.

## Scope

* What is included

  * Update/create BigQuery tables:

    * `packfinderz_analytics.marketplace_events`
    * `packfinderz_analytics.ad_event_facts`
  * Ensure both tables are append-only and partitioned by `DATE(occurred_at)`
  * Marketplace table includes flattened monetary fields and buyer geo fields, plus `items` JSON array and optional `payload` JSON
  * Ad table includes `type` (impression|click|conversion|charge), `cost_cents`, attribution fields, and optional location + `payload` JSON
  * Provide checked-in schema definitions/commands used to create/update tables (e.g., `bq mk` / `bq update` commands or schema JSON files)
* What is explicitly NOT included

  * Implementing analytics-worker ingestion logic
  * Implementing query services or API endpoints
  * Backfill of historical data

## Acceptance Criteria

* [ ] BigQuery tables exist (or are updated) with all columns specified in the canonical spec:

  * `marketplace_events` includes: `event_id`, `event_type`, `occurred_at`, `checkout_group_id`, `order_id`, `buyer_store_id`, `vendor_store_id`, `buyer_zip`, `buyer_lat`, `buyer_lng`, `subtotal_cents`, `discounts_cents`, `tax_cents`, `transport_fee_cents`, `gross_revenue_cents`, `refund_cents`, `net_revenue_cents`, `attributed_ad_id`, `items` (JSON), `payload` (JSON optional).
  * `ad_event_facts` includes: `event_id`, `occurred_at`, `ad_id`, `vendor_store_id`, `buyer_store_id`, `type`, `cost_cents`, `attributed_order_id`, optional buyer geo fields, `payload` (JSON).
* [ ] Both tables are partitioned by `DATE(occurred_at)` and are append-only (no UPDATE/DELETE workflows required by design).

## Dependencies

* Blocked by:

  * None
* Blocks:

  * PF-XXX (BigQuery writer)
  * PF-XXX (Marketplace mapper)
  * PF-XXX / PF-XXX (Query services)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Tables: `packfinderz_analytics.marketplace_events`, `packfinderz_analytics.ad_event_facts`
  * Writer: `cmd/analytics-worker` will insert rows into these tables
  * Partitioning: `DATE(occurred_at)`
* Assumptions (explicitly marked)

  * **ASSUMPTION:** The project uses a repo-tracked approach for BQ schema management (schema JSON + CLI commands) since Goose does not manage BigQuery.
  * **ASSUMPTION:** Existing columns like `attributed_ad_click_id` may still exist in older tables; they can remain but must not be required by the new ingestion/query paths.

## Out of Scope

* Data backfill, dedupe jobs, or query optimization beyond schema correctness

---

# Title [PF-182a]: Define analytics enums and DTOs for Pub/Sub envelope, BQ rows, and queries

## Type

Chore

## Description

Create a single, canonical set of Go types for analytics message envelopes, event types, and BigQuery row DTOs to eliminate stringly-typed routing and reduce mapping errors across outbox → pubsub → analytics-worker → BigQuery → query services.

## Scope

* What is included

  * Go types for:

    * Analytics Pub/Sub envelope (fields: `event_id`, `event_type`, `aggregate_type`, `aggregate_id`, `occurred_at`, `payload`)
    * Analytics event type enum/constants for MVP (`order_created`, `order_paid`, `cash_collected`, `order_canceled`, `order_expired`, future `refund_initiated`; ads: `ad_impression`, `ad_click`, `ad_daily_charge_recorded`)
    * BQ row DTOs:

      * `MarketplaceEventRow` (flattened fields + `Items` JSON + optional `Payload`)
      * `AdEventFactRow` (fields + optional `Payload`)
    * Query request/response DTOs used by query services (marketplace + ad)
  * Shared helpers for timestamp selection used in ingestion (revenue timestamp fallback rules)
* What is explicitly NOT included

  * Handler implementations
  * BigQuery write client
  * SQL query implementations

## Acceptance Criteria

* [ ] `MarketplaceEventRow` and `AdEventFactRow` structs fully represent the canonical BigQuery schemas (PF-181: see `infra/bigquery/schemas/v2/*.json` for v2 conocal BQ schemas).
* [ ] A single envelope DTO exists and is used by analytics-worker consumer routing and handler signatures.
* [ ] Event type constants/enums cover all MVP event types referenced by the canonical spec.

## Dependencies

* Blocked by:

  * None
* Blocks:

  * PF-XXX (Handler switch)
  * PF-XXX / PF-XXX (Mapping + attribution)
  * PF-XXX / PF-XXX (Query services)
  * PF-XXX (Endpoints)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Pub/Sub envelope must match canonical JSON contract
  * Types should live under `internal/analytics/...` (or equivalent) to avoid circular deps
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Existing outbox envelope publisher already produces fields required by the canonical analytics envelope; this ticket focuses on consumer-side DTOs.

## Out of Scope

* Adding new domain events beyond the canonical spec
* Multi-touch attribution DTOs

---

# Title [PF-183]: Implement cmd/analytics-worker Pub/Sub consumer with Redis idempotency gate

## Type

Feature

## Description

Create the dedicated analytics worker binary `cmd/analytics-worker` that subscribes to `analytics-sub`, deserializes the analytics envelope, performs Redis idempotency checks, and dispatches processing to analytics handlers. This is the canonical ingestion entrypoint for analytics.

## Scope

* What is included

  * New binary: `cmd/analytics-worker`
  * Pub/Sub subscription consumption from `analytics-sub`
  * Envelope unmarshal into canonical DTO (PF-XXX)
  * Redis idempotency check:

    * key: `pf:evt:processed:analytics:<event_id>`
    * TTL: `PACKFINDERZ_EVENTING_IDEMPOTENCY_TTL` (default 720h)
  * Ack/nack strategy:

    * ACK if already processed (idempotency key exists)
    * NACK / retry on transient processing errors
    * Fail-fast behavior for invalid envelope payloads (decide retryability)
  * Structured logging including `event_id` correlation
* What is explicitly NOT included

  * Implementing individual event handlers (PF-XXX)
  * Writing to BigQuery (PF-XXX)
  * SQL query layer (PF-XXX/1611)

## Acceptance Criteria

* [ ] Worker successfully connects to Pub/Sub subscription `analytics-sub` and processes at least one message end-to-end through routing (handler stub OK).
* [ ] Idempotency key is written to Redis on successful processing and prevents duplicate processing on redelivery.
* [ ] Worker ACKs messages when idempotency key exists (no additional side effects).

## Dependencies

* Blocked by:

  * PF-XXX (Analytics envelope DTO + event type constants)
* Blocks:

  * PF-XXX (Handler switch wired into worker)
  * PF-XXX (BigQuery writer integration)
  * PF-XXX (Ad charge ingestion) if emitted into this subscription

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Pub/Sub: `analytics-sub`
  * Redis: idempotency keys with configured TTL
  * Worker: long-running process with graceful shutdown handling
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Pub/Sub topic/subscription already exists and is reachable from worker runtime.
  * **ASSUMPTION:** Redis is available to worker (same as other workers in stack).

## Out of Scope

* DLQ implementation for invalid event types (unless already standard in the project)
* High-throughput batching optimization

---

# Title [PF-184]: Implement analytics-worker routing switch and event handler interface contracts

## Type

Task

## Description

Implement the analytics-worker routing layer that dispatches incoming envelopes by `event_type` to dedicated handler functions. Provide a clean handler interface boundary (no god-function) consistent with the canonical spec event taxonomy.

## Scope

* What is included

  * A router module (e.g., `internal/analytics/consumer`) that:

    * validates `event_type` is supported for analytics
    * dispatches to the correct handler
  * Handler stubs/implementations for MVP event types:

    * `order_created`
    * `order_paid`
    * `cash_collected`
    * `order_canceled`
    * `order_expired`
    * ads: `ad_impression`, `ad_click`, `ad_daily_charge_recorded`
  * Clear handler signatures that accept:

    * envelope metadata
    * typed payload DTO (as applicable)
    * writer interfaces (BQ writer) as dependency injection
* What is explicitly NOT included

  * Full payload-to-row mapping logic (PF-XXX)
  * Token attribution decode + conversion logic (PF-XXX)
  * BigQuery writer implementation (PF-XXX)

## Acceptance Criteria

* [ ] Unsupported `event_type` is rejected with a deterministic error path (classified retryable vs non-retryable).
* [ ] Supported `event_type` routes to the correct handler stub with typed payload unmarshaled.
* [ ] Code structure isolates each handler to its own file/package (no monolithic switch body).
* [ ] Remember, `order_created` events can also tricker ad analytics DB writes to BQ if the order contains attribution tag `order` level or `order_line_item` level

## Dependencies

* Blocked by:

  * PF-XXX (event type constants + payload DTOs)
  * PF-XXX (worker binary / consumer skeleton)
* Blocks:

  * PF-XXX (Marketplace mapping integration)
  * PF-XXX (Attribution conversion integration)
  * PF-XXX (Writer interface integration)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Router invoked from `cmd/analytics-worker` message loop
  * Payload unmarshaling must follow canonical payload contracts for each event type
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Event payloads are strictly event-specific and do not share a single “union” schema; router selects the DTO per type.

## Out of Scope

* Adding new event types beyond the canonical list
* Backward-compat support for older payload formats unless explicitly required

---

# Title [PF-185]: Implement BigQuery write client with retries and JSON serialization helpers

## Type

Feature

## Description

Implement a reusable BigQuery writer used by analytics-worker handlers to insert rows into `marketplace_events` and `ad_event_facts`. Writer must support reliable inserts, consistent JSON serialization, and bounded batching (if used).

## Scope

* What is included

  * A BigQuery writer package (e.g., within existing `pkg/bigquery`) that supports:

    * inserting `MarketplaceEventRow` into `marketplace_events`
    * inserting `AdEventFactRow` into `ad_event_facts`
    * consistent encoding for JSON columns (`items`, `payload`)
    * retry policy for transient BigQuery errors
    * optional small batching per message or per handler invocation
  * Interface abstraction so handlers depend on `Writer` interface not concrete client
* What is explicitly NOT included

  * Query services (PF-XXX/1611)
  * Backfill scripts (PF-XXX)
  * Deduplication jobs in BigQuery

## Acceptance Criteria

* [ ] Writer can insert a valid `marketplace_events` row with embedded `items` JSON.
* [ ] Writer can insert a valid `ad_event_facts` 
* [ ] Transient BigQuery insertion failures are retried according to configured policy; non-retryable errors surface to handler.

## Dependencies

* Blocked by:

  * PF-XXX (BQ schema ready)
  * PF-XXX (Row DTOs)
* Blocks:

  * PF-XXX / PF-XXX / PF-XXX (handlers that write to BQ)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * BigQuery dataset: `packfinderz_analytics`
  * Tables: `marketplace_events`, `ad_event_facts`
  * JSON columns must use BigQuery JSON type semantics
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Project already has GCP auth/config conventions used by other GCP clients; follow same env var/SA patterns.
  * **ASSUMPTION:** The file and helpers may exist. Delete & Overwrite or refactor and extend existing funcitons. 

## Out of Scope

* BigQuery MERGE/upsert logic (append-only is canonical)
* Exactly-once guarantees beyond Redis idempotency + at-least-once delivery

---

# Title [PF-186]: Implement marketplace_events row mapping for order_created payload

## Type

Feature

## Description

Map the canonical `order_created` payload (order snapshot model `VednorOrder`) into a flattened `marketplace_events` BigQuery row with embedded `items` JSON, buyer geo fields, and monetary fields. This powers marketplace dashboard queries without Postgres lookups.

## Scope

* What is included

  * Parse `order_created` payload into DTO
  * Populate `marketplace_events` row fields:

    * identifiers: `event_id`, `event_type`, `occurred_at`, `checkout_group_id`, `order_id`, `buyer_store_id`, `vendor_store_id`
    * geo: `buyer_zip`, `buyer_lat`, `buyer_lng` from `shipping_address`
    * money: `subtotal_cents`, `discounts_cents`, `tax_cents`, `transport_fee_cents`, `gross_revenue_cents (= total_cents)`, `refund_cents (=0)`, `net_revenue_cents (= total_cents)`
    * embed `items` JSON array using required shape (product_id/title/category/classification/qty/unit_price/line_total/discount)
    * optional `payload` JSON copy (if included in schema)
  * Write row via BigQuery writer
* What is explicitly NOT included

  * Mapping for other event types (`order_paid`, `cash_collected`, `order_canceled`, `order_expired`) beyond what is required to insert an event row
  * Attribution token decoding + ad conversion record creation (PF-XXX)
  * Query services

## Acceptance Criteria

* [ ] Given a valid `order_created` message payload, the handler writes exactly one `marketplace_events` row with correct flattened values and `items` JSON array.
* [ ] `gross_revenue_cents` equals payload `total_cents`, and `net_revenue_cents` equals `gross_revenue_cents` while refunds are not implemented (refund=0).
* [ ] `buyer_zip`, `buyer_lat`, `buyer_lng` are populated from `shipping_address` when present; failures are handled deterministically.

## Dependencies

* Blocked by:

  * PF-XXX (payload DTOs + row DTOs)
  * PF-XXX (BQ writer)
  * PF-XXX (handler routing)
* Blocks:

  * PF-XXX (Marketplace query service depends on this data existing)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Handler: `order_created`
  * Table: `packfinderz_analytics.marketplace_events`
  * Embedded `items` JSON used for UNNEST-based top products/categories queries
* Assumptions (explicitly marked)

  * **ASSUMPTION:** `shipping_address` in payload includes `postal_code`, `lat`, `lng` as shown in canonical payload contract.
  * **ASSUMPTION:** `tax_cents` and `transport_fee_cents` may be 0 in early MVP; still stored.

## Out of Scope

* Revalidation of pricing/availability (analytics trusts snapshot)
* Separate line-item fact table

---

# Title [PF-187]: Implement attribution token decoding and ad conversion record generation [INCOMPLETE ❌]

## Type

Feature

## Description

Implement the token receipt attribution model for analytics: decode `attributed_token` (order level) || `attributed_token` (`order_line_item` level) from order snapshot payloads and generate `ad_event_facts` conversion rows with attributed revenue based on token target type (store vs product). This enables ROAS computation in BigQuery.

## Scope

* What is included

  * Implement `internal/analytics/attribution` package:

    * token decode to extract minimum fields: `token_id`, `ad_id`, `target_type`, `target_id`, `buyer_store_id`, `event_type`, `occurred_at` (and any other claim fields used in payload)
  * Attribution logic:

    * If `target_type == store`: attribute full order `total_cents`
    * If `target_type == product`: attribute sum of `items[].line_total_cents` where `product_id == target_id`
  * Generate and insert `ad_event_facts` row(s) with:

    * `type=conversion`
    * `ad_id`, `vendor_store_id` (from payload), `buyer_store_id`
    * `attributed_order_id=order_id`
    * `cost_cents=0` (conversion record; spend comes from charge events)
    * `payload` includes: `order_id`, `checkout_group_id`, `target_type`, `target_id`, `attributed_revenue_cents`, `token_id`, `creative_id` (if present), `placement` (if present)
  * Define deterministic handling of multiple tokens
* What is explicitly NOT included

  * Token validation at checkout time (already domain responsibility)
  * Conversion rate / funnel analytics
  * Multi-touch attribution modeling

## Acceptance Criteria

* [ ] When `order_created` payload contains non-empty `attributed_ad_tokens[]`, analytics-worker inserts at least one `ad_event_facts` row with `type=conversion` and correct attributed revenue cents per token rules.
* [ ] When no tokens exist, analytics-worker writes no `ad_event_facts` rows but still writes marketplace event row.
* [ ] Multiple token behavior is deterministic and documented in code (e.g., last applicable token by token.occurred_at per target, or another rule explicitly implemented).

## Dependencies

* Blocked by:

  * PF-XXX (payload + row DTOs)
  * PF-XXX (BQ writer)
  * PF-XXX (order_created mapping provides items + totals)
* Blocks:

  * PF-XXX (Ad query service ROAS requires conversion rows)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Tokens originate from checkout snapshot: `attributed_ad_tokens[]`
  * Table: `packfinderz_analytics.ad_event_facts`
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Token decoding does not require signature verification in MVP unless mandated; if verification is required, the signing key source must be provided/configured.
  * **ASSUMPTION:** If multiple tokens exist, apply “last applicable token by token.occurred_at per target”; if product+store tokens both exist, both may emit conversion rows (per current spec).

## Out of Scope

* Windowing/expiration enforcement beyond what is encoded in checkout validation
* “Prefer click over impression” rule unless explicitly implemented as the deterministic rule

---

# Title [PF-188]: Implement marketplace_events handling for order_paid and cash_collected events 

## Type

Task

## Description

Insert marketplace analytics rows for payment lifecycle events (`order_paid`, `cash_collected`) using the canonical payload contracts. These events drive revenue time series metrics (paid/cash collected timestamps) per the analytics timestamp rules.

## Scope

* What is included

  * Implement handlers for:

    * `order_paid` payload
    * `cash_collected` payload
  * Insert one `marketplace_events` row per event with:

    * `event_id`, `event_type`, `occurred_at`
    * `order_id`, `vendor_store_id`, `buyer_store_id` (as available in payload)
    * revenue-related fields (at minimum `gross_revenue_cents` / `net_revenue_cents` using `amount_cents` where provided)
  * Document how these rows are used by query service to determine “revenue timestamp” series
* What is explicitly NOT included

  * Updating or mutating prior marketplace rows (append-only)
  * Refund events (future)
  * Fulfillment analytics

## Acceptance Criteria

* [x] Processing an `order_paid` message results in exactly one `marketplace_events` row inserted with `event_type=order_paid` and revenue cents populated from payload.
* [x] Processing a `cash_collected` message results in exactly one `marketplace_events` row inserted with `event_type=cash_collected` and revenue cents populated from payload.
* [ ] The query layer can distinguish created vs paid vs cash-collected rows via `event_type` and `occurred_at` (no reliance on publish time).

## Dependencies

* Blocked by:

  * PF-XXX (BQ writer)
  * PF-XXX (handlers wired)
  * PF-XXX (schema updated)
* Blocks:

  * PF-XXX (Marketplace query service requires these event types for revenue time series)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Handler inputs follow canonical payloads for `order_paid` and `cash_collected`
  * Table: `packfinderz_analytics.marketplace_events`
* Assumptions (explicitly marked)

  * **ASSUMPTION:** `amount_cents` in `order_paid` and `cash_collected` payloads matches gross revenue cents for analytics time series.
  * **ASSUMPTION:** If geo + items are not present in these payloads, they remain null in these rows; queries must not depend on them for revenue series.

## Out of Scope

* Merging events into a single “order fact row” (MERGE/upsert) — append-only is canonical

---

# Title [PF-189]: Implement marketplace_events handling for order_canceled and order_expired events

## Type

Task

## Description

Insert marketplace analytics rows for order termination events (`order_canceled`, `order_expired`) so dashboards can exclude these from revenue metrics and optionally support dropped-order reporting later.

## Scope

* What is included

  * Implement handlers for:

    * `order_canceled` payload
    * `order_expired` payload
  * Insert one `marketplace_events` row per event with:

    * `event_id`, `event_type`, `occurred_at`
    * `order_id`, `buyer_store_id`, `vendor_store_id`
    * include `payload` JSON with cancellation/expiry metadata (reason/ttl_days) if schema supports it
* What is explicitly NOT included

  * Any revenue adjustment calculations
  * Refunds
  * “Dropped orders” KPI UI (can be added later)

## Acceptance Criteria

* [x] Processing `order_canceled` inserts exactly one marketplace row with `event_type=order_canceled` and includes cancellation metadata (reason) in payload if available.
* [x] Processing `order_expired` inserts exactly one marketplace row with `event_type=order_expired` and includes expiry metadata (ttl_days) in payload if available.

## Dependencies

* Blocked by:

  * PF-XXX (BQ writer)
  * PF-XXX (handler routing)
* Blocks:

  * PF-XXX (Marketplace query service needs cancellation/expiry exclusion logic)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Table: `packfinderz_analytics.marketplace_events`
  * Queries must exclude canceled/expired rows from revenue metrics per spec
* Assumptions (explicitly marked)

  * **ASSUMPTION:** These event payloads do not include amounts; amounts are not required for termination events.

## Out of Scope

* Implementing a “dropped orders over time” dashboard metric (future)

---

# Title [PF-190]: Emit ad_daily_charge_recorded analytics events from nightly billing job and write charge rows to ad_event_facts [INCOMPLETE ❌]

## Type

Feature

## Description

Integrate nightly ad billing (based on Redis impression/click counters) with analytics by emitting a domain/outbox event `ad_daily_charge_recorded` and ingesting it into BigQuery as `ad_event_facts` rows with `type=charge` and `cost_cents`. This is the canonical spend source for ROAS.

## Scope

* What is included

  * Update nightly billing job to:

    * compute daily `cost_cents` per `ad_id` (and per billing type as applicable)
    * emit `ad_daily_charge_recorded` via outbox (domain commit)
  * Implement analytics-worker handler for `ad_daily_charge_recorded`:

    * writes one `ad_event_facts` row per ad/day with:

      * `type=charge`
      * `cost_cents`
      * `occurred_at` set to charge timestamp (end-of-day or job run time)
      * `ad_id`, `vendor_store_id`
      * optional payload for debug (counters, rate, day)
* What is explicitly NOT included

  * Reworking billing correctness or ledger accounting beyond existing nightly job
  * Impression/click ingestion mechanics if they are not already producing domain events
  * Real-time spend updates

## Acceptance Criteria

* [ ] Nightly billing job emits `ad_daily_charge_recorded` outbox events with `ad_id`, `vendor_store_id`, `cost_cents`, and `occurred_at`.
* [ ] Analytics-worker consumes `ad_daily_charge_recorded` and inserts `ad_event_facts` rows with `type=charge` and correct `cost_cents`.
* [ ] Spend in ad queries can be computed as `SUM(cost_cents)` filtered by `ad_id` and timeframe.

## Dependencies

* Blocked by:

  * PF-XXX (BQ writer)
  * Existing nightly billing job implementation (outside this phase but assumed present)
* Blocks:

  * PF-XXX (Ad query service requires spend rows)
  * PF-XXX (Ad endpoints require query service)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Nightly billing job reads Redis counters, computes charges, writes outbox event
  * Analytics handler inserts into `packfinderz_analytics.ad_event_facts`
* Assumptions (explicitly marked)

  * **ASSUMPTION:** There is an existing nightly job context that can emit domain/outbox events; if not, emitting outbox events must be added within the billing job transaction boundary.

## Out of Scope

* CPM/CPC pricing model redesign
* Deduping charge events in BQ beyond idempotency keying

---

# Title [PF-191]: Implement Marketplace Analytics query service using BigQuery SQL

## Type

Feature

## Description

Implement a marketplace analytics query service that reads BigQuery `marketplace_events` and returns dashboard-ready data: orders/revenue/discount/net time series, top products/categories/ZIPs, AOV, and new vs returning buyers for a selected timeframe.

## Scope

* What is included

  * `internal/analytics/query/marketplace_service.go` with BQ query methods:

    1. Orders over time (created event rows)
    2. Gross revenue over time (paid/cash_collected rows per rules)
    3. Discounts over time
    4. Net revenue over time
    5. Top 5 products by revenue via `UNNEST(items)`
    6. Top 5 categories by revenue via `UNNEST(items)`
    7. Top 5 ZIPs (bar chart MVP)
    8. AOV (gross revenue / distinct order_id) for timeframe
    9. New vs returning customers (buyer_store_id prior paid before start)
  * Timeframe inputs: `day/7d/30d/90d/1y/custom` and `store_id` filtering (active store context)
  * Response DTOs shaped for frontend consumption
    * Time series data needs to be {date, value}
    * Top 5 need to be {label, value}
    * For now zips will be top 5 {label, value}, hwoever, soon we will return ALL lat/long to publish on a map chart front end to give a heatmap of the state. 
* What is explicitly NOT included

  * Admin analytics endpoints (non-MVP)
  * Conversion rate funnel analytics
  * Fulfillment analytics

## Acceptance Criteria

* [ ] Service returns correct values for all listed marketplace KPIs for a given timeframe and store context using only BigQuery.
* [ ] Top products/categories queries correctly parse/UNNEST `items` JSON and aggregate revenue.
* [ ] New vs returning logic is implemented as “has any paid/cash_collected order prior to timeframe start” for `buyer_store_id`.

## Dependencies

* Blocked by:

  * PF-XXX (BQ schema)
  * PF-XXX / PF-XXX / PF-XXX (marketplace events are written)
* Blocks:

  * PF-XXX (Marketplace analytics API endpoints)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Table: `packfinderz_analytics.marketplace_events`
  * Partition pruning by `occurred_at` should be used in queries
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Store context filter is implemented as either `vendor_store_id=<activeStoreId>` for vendor view or `buyer_store_id=<activeStoreId>` for buyer view; exact behavior must match existing dashboard expectations.

## Out of Scope

* Materialized views or pre-aggregations
* Backfill of missing historical events

---

# Title [PF-192]: Implement Ad Analytics query service using BigQuery SQL [INCOMPLETE ❌]

## Type

Feature

## Description

Implement an ad analytics query service that reads BigQuery `ad_event_facts` (spend/impressions/clicks/charges) and joins against `marketplace_events` (conversion attribution) to compute ad KPIs including ROAS.

## Scope

* What is included

  * `internal/analytics/query/ad_service.go` with BQ query methods:

    * Spend: `SUM(cost_cents)` where `type=charge`
    * Impressions/clicks: counts by `type`
    * CPM/CPC derived for timeframe:

      * CPM = spend / (impressions/1000)
      * CPC = spend / clicks
    * Avg daily impressions/clicks (daily grouping)
    * ROAS:

      * attributed gross revenue from `marketplace_events` rows where `attributed_ad_id = ad_id` (or from `ad_event_facts` conversion payload if used)
      * divided by spend
  * Timeframe inputs and `ad_id` parameterization
  * Response DTOs for frontend
* What is explicitly NOT included

  * Multi-touch attribution
  * Conversion funnels beyond attribution-based conversions
  * Real-time spend (nightly charges are canonical)

## Acceptance Criteria

* [ ] Service returns spend, impressions, clicks, CPM, CPC, and ROAS for a given `ad_id` and timeframe using BigQuery.
* [ ] ROAS is computed using gross revenue attribution per canonical spec and spend from `type=charge` rows.
* [ ] Daily series queries bucket by date consistently (e.g., `DATE(occurred_at)`).

## Dependencies

* Blocked by:

  * PF-XXX (BQ schema)
  * PF-XXX (conversion rows)
  * PF-XXX (charge rows for spend)
* Blocks:

  * PF-XXX (Ad analytics API endpoints)

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * Tables: `packfinderz_analytics.ad_event_facts`, `packfinderz_analytics.marketplace_events`
  * Ensure partition filters on `occurred_at` for performance
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Attribution revenue source is either:

    * `marketplace_events.attributed_ad_id` field, OR
    * `ad_event_facts` conversion payload `attributed_revenue_cents`.
      The chosen approach must be implemented consistently across ingestion + query.

## Out of Scope

* Attribution windowing
* User-level attribution analytics

---

# Title [PF-193]: Add marketplace analytics API endpoints backed by BigQuery query service

## Type

Feature

## Description

Expose client-facing marketplace analytics endpoints that return dashboard-ready payloads using the Marketplace Analytics query service. Endpoints must enforce auth and store scoping via `activeStoreId` context.

## Scope

* What is included

  * Controller + routes for marketplace analytics (exact route names may vary; implement per existing API conventions):

    * Summary payload endpoint (recommended for frontend simplicity), OR
    * Separate endpoints for summary vs timeseries metrics
  * Validate timeframe query params and apply defaults
  * Enforce store scoping using JWT `activeStoreId` and store type (vendor/buyer) rules
  * Response DTO matches frontend needs for:

    * orders/revenue/discount/net time series
    * top products/categories/ZIPs
    * AOV
    * new vs returning
* What is explicitly NOT included

  * Admin analytics endpoints (explicitly non-MVP)
  * Ad endpoints (PF-XXX)
  * Fulfillment analytics endpoints

## Acceptance Criteria

* [ ] Authorized user can fetch marketplace analytics for their active store and timeframe.
* [ ] Endpoint responses are computed solely from BigQuery via `MarketplaceAnalyticsService`.
* [ ] Invalid timeframe inputs return validation errors consistent with API envelope standards.

## Dependencies

* Blocked by:

  * PF-XXX (Marketplace query service)
* Blocks:

  * Frontend integration for marketplace analytics dashboard

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * API uses `internal/analytics/query/marketplace_service.go`
  * JWT `activeStoreId` used for scoping
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Existing auth middleware injects store context and store type; endpoint should follow established patterns in repo.

## Out of Scope

* Caching layer
* Rate limiting beyond existing middleware

---

# Title [PF-194]: Add ad analytics API endpoints backed by BigQuery query service [INCOMPLETE ❌]

## Type

Feature

## Description

Expose client-facing ad analytics endpoints that return ad dashboard metrics (spend, impressions, clicks, CPM/CPC, ROAS, time series) using the Ad Analytics query service. Endpoints must enforce that the requesting store owns/controls the ad.

## Scope

* What is included

  * Controller + routes for ad analytics:

    * `GET /v1/analytics/ads/:ad_id/summary?timeframe=...`
    * optional `GET /v1/analytics/ads/:ad_id/timeseries?...`
  * Validate timeframe params
  * Enforce authorization:

    * advertiser store must match `activeStoreId` (vendor store)
  * Responses include:

    * spend (from charge rows)
    * impressions/clicks
    * derived CPM/CPC
    * ROAS
    * daily series aggregates
* What is explicitly NOT included

  * Creating/updating ads
  * Conversion funnel analytics
  * Admin or cross-tenant analytics access

## Acceptance Criteria

* [ ] Authorized advertiser store can fetch ad analytics for an ad it owns.
* [ ] Endpoint returns spend/ROAS/impressions/clicks/CPM/CPC computed via `AdAnalyticsService`.
* [ ] Unauthorized access to other stores’ ads is denied.

## Dependencies

* Blocked by:

  * PF-XXX (Ad query service)
* Blocks:

  * Frontend integration for ad analytics dashboard

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * API uses `internal/analytics/query/ad_service.go`
  * Ownership check likely references Postgres ads table or claim-based ownership
* Assumptions (explicitly marked)

  * **ASSUMPTION:** There is an authoritative mapping of `ad_id → vendor_store_id` available (likely Postgres) to enforce ownership; query service itself remains BigQuery-only for metrics.

## Out of Scope

* Admin analytics access
* Bulk ad analytics across all ads in one request unless already required

---

# Title [PF-195]: Build analytics ingestion and query test harness plus one-time BigQuery backfill script [INCOMPLETE ❌]

## Type

Task

## Description

Add automated tests for analytics ingestion/mapping and query-layer correctness, and implement a one-time backfill script to populate BigQuery from existing Postgres orders so the dashboard has immediate data after schema changes.

## Scope

* What is included

  * Unit tests for:

    * marketplace mapping correctness (money math, geo, items JSON shape)
    * attribution token decoding and conversion row generation
    * query builders (SQL correctness, parameter binding)
  * Integration test harness (choose one approach):

    * publish sample messages into Pub/Sub emulator, OR
    * invoke handlers directly with mock BQ writer
  * One-time backfill script:

    * paginate Postgres `vendor_orders` and related line item snapshot data
    * build rows using the same mapping logic as ingestion
    * insert into BigQuery with deterministic event ids
* What is explicitly NOT included

  * Reconciliation tooling between Postgres and BigQuery
  * Continuous backfill jobs
  * Performance benchmarking suite

## Acceptance Criteria

* [ ] Mapping unit tests cover at least: gross/net/discount math, items JSON required fields, geo extraction.
* [ ] Attribution tests cover: no tokens, store-level token, product-level token, multiple tokens deterministic behavior.
* [ ] Backfill script successfully inserts BigQuery rows for a sample set of historical orders with deterministic `event_id` to support reruns without duplicating results.

## Dependencies

* Blocked by:

  * PF-XXX (BQ writer) and PF-XXX/1607 mapping logic
  * Postgres access patterns for reading historical orders
* Blocks:

  * Confident rollout of analytics dashboards with historical data

## Technical Notes

* Relevant services, tables, APIs, queues, or workers

  * BigQuery tables: `marketplace_events`, `ad_event_facts`
  * Postgres tables: vendor orders + order line items snapshot (exact table names per existing models)
* Assumptions (explicitly marked)

  * **ASSUMPTION:** Historical orders have sufficient snapshot fields to build `items` JSON and monetary fields required by schema.
  * **ASSUMPTION:** Deterministic backfill event ids use a stable naming scheme (e.g., `backfill_order_<order_id>_<created_at>`).

## Out of Scope

* Automated ongoing backfill for late-arriving data
* Data quality dashboards
